{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Lazy evaluation\n\nA powerful feature of xarray is `lazy evaluation`_. Lazy evaluation means that\nexpressions are delayed until the results are needed. In a nutshell, lazy\nevaluation has the following advantages for us:\n\n    * Lazy evaluation frees us from having to load all data into memory in one\n      go. We can work with larger-than-memory datasets without having to\n      manually divide the datasets into smaller, manageable pieces.\n    * Lazy evaluation allows us to easily parallellize computations. This can\n      speed up computations considerably depending on the available number of\n      cores.\n\nLazy evaluation can be contrasted with eager evaluation. In eager evaluation\nwith xarray, all data always exists in RAM memory, and computations are\nperformed immediately. Typical cases of when results are needed is when data\nhas to be visualized or when the data has to be saved to disk.\n\nWe will explain these concepts with a number of examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import dask.array\nimport numpy as np\nimport xarray as xr\n\nimport imod\n\nlazy_da = xr.DataArray(dask.array.ones(5), dims=[\"x\"])\nlazy_result = lazy_da + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we run the lines above, we evaluate the expression ``lazy_result =\nlazy_da + 1``. In eager evaluation, this means a value of 1 is added to every\nelement; in delayed evaluation, the expression is stored instead in a graph\nof tasks. In xarray, delayed evaluation is taken care of by dask, an open\nsource Python library for (parallel) scheduling and computing.\n\nWe can identify the DataArray as lazy, since its ``.data`` attribute shows a\ndask array rather than a numpy array.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lazy_result.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can take a look at the graph of tasks as follows:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lazy_result.data.visualize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>The ``show`` function here is required for the online documentation. If\n  you're running the examples interactively, the line above will show you a\n  picture of the graph, and defining and calling ``show()`` can be ommitted.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def show():\n    # Show the latest png generated by dask.visualize with matplotlib. This is\n    # required because the sphinx image scraper currently only supports\n    # matplotlib and mayavi and will not capture the output of\n    # dask.visualize().\n    import matplotlib.image as mpimg\n    import matplotlib.pyplot as plt\n\n    _, ax = plt.subplots()\n    ax.set_axis_off()\n    ax.imshow(mpimg.imread(\"mydask.png\"))\n\n\nshow()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see a very simple graph with two tasks:\n\n* create the array of ones;\n* add a value of 1 to every element.\n\nLet's constrast this with eager evaluation. Rather than creating the data\nas a dask array, we will create it as an ordinary numpy array.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "eager_da = xr.DataArray(np.ones(5), dims=[\"x\"])\neager_result = eager_da + 1\neager_result.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data is an numpy array. Since the actual values of the elements of the\narray are in memory, we also get a preview of the element values.\n\nSee also the xarray documentation on `parallel computing with dask`_.\n\n## Delayed computation and (i)MODFLOW files\n\nThe input and output functions in the imod package have been written to work\nwith xarray's delayed evaluation. This means that opening IDFs or MODFLOW6\nwill not load the data from the files into memory until they are needed.\n\nLet's grab some example results, store them in a temporarily directory, and\nopen them:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result_dir = imod.util.temporary_directory()\nimod.data.twri_output(result_dir)\nhead = imod.mf6.open_hds(result_dir / \"twri.hds\", result_dir / \"twri.grb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All time steps are available, but not loaded. Instead, a graph of tasks has\nbeen defined. Let's take a look at the tasks:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "head.data.visualize()\nshow()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We might want to perform a computation for every time step. Let's assume we\nwould want to compute the mean head for every time step:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mean_head = head.mean([\"y\", \"x\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can take a look at the graph, and see that the dask are fully independent\nof each other:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mean_head.data.visualize()\nshow()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This means the computation is `embarrassingly parallel`_. Every task can be\ncomputed independently of the other by the individual CPUs of your computer.\n\nDask will automatically distribute tasks over your CPUs, and manage the tasks\nso that memory usage does not exceed maximum RAM. This goes well most of the\ntime, but not always; refer to the `Pitfalls`_ section below.\n\n## Chunk size and memory layout\n\nComputer memory and data storage are best thought of a one-dimensional arrays\nof bits (even though the hardware might be actually `multi-dimensional`_\nbehind the scenes).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pitfalls\n\nThere are a number of pitfalls to be aware of when working with dask and lazy\nevaluation. A primary pitfall is not taking the chunk (and memory) layout\ninto mind. For working with MODFLOW models, this is typically operations that\noperate **over** the time dimension. Because of the file formats, MODFLOW\ndata is chunked over the time dimension.\n\nConcretely, this means that the following code can be very slow for large\ndatasets:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "points = [(0.0, 0.0), (1.0, 1.0), (2.0, 2.0)]\nsamples = [head.sel(x=x, y=y, method=\"nearest\") for (x, y) in points]\n\nsamples[0].data.visualize()\nshow()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The reason is that in the line above, a single x-y point is extracted at a\ntime. To extract this single point, the entire dataset must be loaded into\nmemory! Generally, the solution is to use a vectorized approach, selecting\nall the points \"at once\".\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = [0.0, 1.0, 2.0]\ny = [0.0, 1.0, 2.0]\nsamples = head.sel(x=x, y=y, method=\"nearest\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In these lines, the chunk is loaded, all point values for a single time step are\nextracted and store; then the next chunk is loaded, etc. Because the call is\nvectorized, dask can optimize the selection, and figure out that it should\nextract all values and load the chunks only once. We can verify this by taking\na look at the task graph again:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "samples.data.visualize()\nshow()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are cases where the data cannot be loaded in a vectorized manner, or\ncases where the scheduling goes haywire. In this case, a straightforward\nsolution is to fallback to eager evaluation (with numpy arrays instead of\ndask arrays). This can be done at any time by simply calling ``.compute()``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "eager_head = head.compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This has the obvious downside that the entire dataset needs to fit in memory.\n\nHowever, the following lines are relatively fast again, since the entire\ndataset has already been loaded into memory this time around:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "points = [(0.0, 0.0), (1.0, 1.0), (2.0, 2.0)]\nsamples = [eager_head.sel(x=x, y=y, method=\"nearest\") for (x, y) in points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Annoyingly, the VSCode interactive Python interpreter does not cooperate well\nwith every `dask scheduler`_. If you're working in VSCode, and either\nperformance seems slow or memory usage is very high, try running your script\noutside of VSCode.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}