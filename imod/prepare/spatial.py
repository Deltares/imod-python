import pathlib
import subprocess

import affine
import dask
import numba
import numpy as np
import pandas as pd
import rasterio
import rasterio.features
import rasterio.warp
import scipy.ndimage
import xarray as xr

import imod


def _reproject_dst(source, src_crs, dst_crs, src_transform):
    """
    Prepares destination transform Affine and DataArray for projection.
    """
    src_height, src_width = source.y.size, source.x.size
    bounds = rasterio.transform.array_bounds(src_height, src_width, src_transform)
    dst_transform, dst_width, dst_height = rasterio.warp.calculate_default_transform(
        src_crs, dst_crs, src_width, src_height, *bounds
    )
    # from: http://xarray.pydata.org/en/stable/generated/xarray.open_rasterio.html
    x, y = (
        np.meshgrid(np.arange(dst_width) + 0.5, np.arange(dst_height) + 0.5)
        * dst_transform
    )
    dst = xr.DataArray(
        data=np.zeros((dst_height, dst_width), source.dtype),
        coords={"y": y[:, 0], "x": x[0, :]},
        dims=("y", "x"),
    )
    return dst_transform, dst


def reproject(
    source,
    like=None,
    src_crs=None,
    dst_crs=None,
    method="nearest",
    use_src_attrs=False,
    src_nodata=np.nan,
    **reproject_kwargs,
):
    """
    Reprojects and/or resamples a 2D xarray DataArray to a 
    different cellsize or coordinate system.

    * To resample to a new cellsize in the same projection: provide only `like`.
    * To only reproject: provide only `src_crs` and `src_crs`.
    * To reproject and resample to a specific domain: provide `src_crs`, `src_crs`, and `like`.
    
    Note: when only `like` is provided, Cartesian (projected) coordinates are a
    ssumed for resampling. In case of non-Cartesian coordinates, specify 
    `src_crs` and `dst_crs` for correct resampling.

    Parameters
    ----------
    source: xarray DataArray
        The DataArray to be resampled and/or reprojected. Must contain dimensions
        `y` and `x`.
    like: xarray DataArray
        Example DataArray that shows what the resampled result should look like 
        in terms of coordinates. Must contain dimensions `y` and `x`.
    src_crs: string, dict, rasterio.crs.CRS
        Coordinate system of `source`. Options:

        * string: e.g. `"+init=EPSG:4326"`
        * dict: e.g. `{"init":"EPSG:4326"}`
        * rasterio.crs.CRS
    dst_crs: string, dict, rasterio.crs.CRS
        Coordinate system of result. Options:

        * string: e.g. `"+init=EPSG:4326"`
        * dict: e.g. `{"init":"EPSG:4326"}`
        * rasterio.crs.CRS
    use_src_attrs: boolean
        If True: Use metadata in `source.attrs`, as generated by `xarray.open_rasterio()`, to do 
        reprojection.
    method: string
        The method to use for resampling/reprojection.
        Defaults to "nearest". GDAL methods are available:

        * nearest
        * bilinear
        * cubic
        * cubic_spline
        * lanczos
        * average
        * mode
        * gauss
        * max
        * min
        * med (50th percentile)
        * q1 (25th percentile)
        * q3 (75th percentile)
    reproject_kwargs: dict, optional
        keyword arguments for `rasterio.warp.reproject()`.

    Returns
    ------- 
    xarray.DataArray
        Resampled/reprojected DataArray.

    Examples
    --------
    Resample a DataArray `a` to a new cellsize, using an existing DataArray `b`:
    
    >>> c = imod.rasterio.resample(source=a, like=b)
    
    Resample a DataArray to a new cellsize of 100.0, by creating a `like` DataArray first:
    (Note that dy must be negative, as is usual for geospatial grids.)
    
    >>> dims = ("y", "x")
    >>> coords = {"y": np.arange(200_000.0, 100_000.0, -100.0), "x": np.arange(0.0, 100_000.0, 100.0)}
    >>> b = xr.DataArray(data=np.empty((200, 100)), coords=coords, dims=dims)
    >>> c = imod.rasterio.resample(source=a, like=b)

    Reproject a DataArray from one coordinate system (WGS84, EPSG:4326) to another (UTM30N, EPSG:32630):

    >>> c = imod.rasterio.resample(source=a, src_crs="+init=EPSG:4326", dst_crs="+init=EPSG:32630")

    Get the reprojected DataArray in the desired shape and coordinates by providing `like`:

    >>> c = imod.rasterio.resample(source=a, like=b, src_crs="+init=EPSG:4326", dst_crs="+init=EPSG:32630")

    Open a single band raster, and reproject to RD new coordinate system (EPSG:28992), without explicitly specifying `src_crs`.
    `src_crs` is taken from `a.attrs`, so the raster file has to include coordinate system metadata for this to work.

    >>> a = xr.open_rasterio("example.tif").squeeze("band")
    >>> c = imod.rasterio.resample(source=a, use_src_attrs=True, dst_crs="+init=EPSG:28992")

    In case of a rotated `source`, provide `src_transform` directly or `use_src_attrs=True` to rely on generated attributes:

    >>> rotated = xr.open_rasterio("rotated_example.tif").squeeze("band")
    >>> c = imod.rasterio.resample(source=rotated, dst_crs="+init=EPSG:28992", reproject_kwargs={"src_transform":affine.Affine(...)})
    >>> c = imod.rasterio.resample(source=rotated, dst_crs="+init=EPSG:28992", use_src_attrs=True)
    """
    assert source.dims == (
        "y",
        "x",
    ), "resample does not support dimensions other than `x` and `y` for `source`."
    if like is not None:
        assert like.dims == (
            "y",
            "x",
        ), "resample does not support dimensions other than `x` and `y` for `like`."
    if use_src_attrs:  # only provided when reproject is necessary
        src_crs = rasterio.crs.CRS.from_string(source.attrs["crs"])
        src_nodata = source.attrs["nodatavals"][0]

    resampling_methods = {e.name: e for e in rasterio.enums.Resampling}

    if isinstance(method, str):
        try:
            resampling_method = resampling_methods[method]
        except KeyError as e:
            raise ValueError(
                "Invalid resampling method. Available methods are: {}".format(
                    resampling_methods.keys()
                )
            ) from e
    elif isinstance(method, rasterio.enums.Resampling):
        resampling_method = method
    else:
        raise TypeError("method must be a string or rasterio.enums.Resampling")

    # Givens: source, like, method. No reprojection necessary.
    if src_crs is None and dst_crs is None:
        if like is None:
            raise ValueError(
                "If crs information is not provided, `like` must be provided."
            )
        if resampling_method == rasterio.enums.Resampling.nearest:
            # this can be handled with xarray
            # xarray 0.10.9 needs .compute()
            # see https://github.com/pydata/xarray/issues/2454
            return source.compute().reindex_like(like, method="nearest")
        else:
            # if no crs is defined, assume it should remain the same
            # in this case use UTM30, ESPG:32630, as a dummy value for GDAL
            # (Any projected coordinate system should suffice, Cartesian plane == Cartesian plane)
            dst = like.copy()
            src_crs = dst_crs = rasterio.crs.CRS.from_epsg(32630)
        src_transform = imod.util.transform(source)
        dst_transform = imod.util.transform(like)

    elif src_crs and dst_crs:
        if use_src_attrs:
            # TODO: modify if/when xarray uses affine by default for transform
            src_transform = affine.Affine(*source.attrs["transform"][:6])
        elif "src_transform" in reproject_kwargs.keys():
            src_transform = reproject_kwargs.pop("src_transform")
        else:
            src_transform = imod.util.transform(source)

        # If no like is provided, just resample to different coordinate system
        if like is None:
            dst_transform, dst = _reproject_dst(source, src_crs, dst_crs, src_transform)
        else:
            dst_transform = imod.util.transform(like)
            dst = like.copy()

    else:
        raise ValueError(
            "At least `like`, or crs information for source and destination must be provided."
        )

    assert src_transform[0] > 0, "dx of 'source' must be positive"
    assert src_transform[4] < 0, "dy of 'source' must be negative"
    assert dst_transform[0] > 0, "dx of 'like' must be positive"
    assert dst_transform[4] < 0, "dy of 'like' must be negative"

    rasterio.warp.reproject(
        source.values,
        dst.values,
        src_transform=src_transform,
        dst_transform=dst_transform,
        src_crs=src_crs,
        dst_crs=dst_crs,
        resampling=resampling_method,
        src_nodata=src_nodata,
        **reproject_kwargs,
    )

    dst.attrs = source.attrs
    dst.attrs["transform"] = dst_transform
    dst.attrs["res"] = (abs(dst_transform[0]), abs(dst_transform[4]))
    dst.attrs["crs"] = dst_crs
    # TODO: what should be the type of "crs" field in attrs?
    # Doesn't actually matter for functionality
    # rasterio accepts string, dict, and CRS object
    return dst


def round_extent(extent, cellsize):
    """Increases the extent until all sides lie on a coordinate
    divisible by cellsize."""
    xmin, ymin, xmax, ymax = extent
    xmin = np.floor(xmin / cellsize) * cellsize
    ymin = np.floor(ymin / cellsize) * cellsize
    xmax = np.ceil(xmax / cellsize) * cellsize
    ymax = np.ceil(ymax / cellsize) * cellsize
    return xmin, ymin, xmax, ymax


def round_z(z_extent, dz):
    """Increases the extent until all sides lie on a coordinate
    divisible by dz."""
    zmin, zmax = z_extent
    zmin = np.floor(zmin / dz) * dz
    zmax = np.ceil(zmax / dz) * dz
    return zmin, zmax


def _fill_np(data, invalid):
    """Basic nearest neighbour interpolation"""
    # see: https://stackoverflow.com/questions/5551286/filling-gaps-in-a-numpy-array
    ind = scipy.ndimage.distance_transform_edt(
        invalid, return_distances=False, return_indices=True
    )
    return data[tuple(ind)]


def fill(da, invalid=None, by=None):
    """
    Replace the value of invalid `da` cells (indicated by `invalid`)
    using basic nearest neighbour interpolation.

    Parameters
    ----------
    da: xr.DataArray with gaps
        array containing missing value
        if one of the dimensions is layer, it will interpolate one layer at a
        a time (2D interpolation over x and y in case of dims == (by, "y", "x")).

    invalid: xr.DataArray
        a binary array of same shape as `da`.
        data value are replaced where invalid is True
        If None (default), uses: `invalid = np.isnan(data)`

    Returns
    -------
    xarray.DataArray
        with the same coordinates as the input.
    """

    out = xr.full_like(da, np.nan)
    if invalid is None:
        invalid = np.isnan(da)
    if by:
        for coordvalue in da[by]:
            d = {by: coordvalue}
            out.sel(d)[...] = _fill_np(da.sel(d).values, invalid.sel(d).values)
    else:
        out.values = _fill_np(da.values, invalid.values)

    return out


def rasterize(geodataframe, like, column=None, fill=np.nan, **kwargs):
    """
    Rasterize a geopandas GeoDataFrame onto the given
    xarray coordinates.

    Parameters
    ----------
    geodataframe : geopandas.GeoDataFrame
    column : str, int, float
        column name of geodataframe to burn into raster
    like : xarray.DataArray
        Example DataArray. The rasterized result will match the shape and
        coordinates of this DataArray.
    fill : float, int
        Fill value for nodata areas. Optional, default value is np.nan.
    kwargs : additional keyword arguments for rasterio.features.rasterize.
        See: https://rasterio.readthedocs.io/en/stable/api/rasterio.features.html#rasterio.features.rasterize

    Returns
    -------
    rasterized : xarray.DataArray
        Vector data rasterized. Matches shape and coordinates of `like`.
    """

    if column is not None:
        shapes = [
            (geom, value)
            for geom, value in zip(geodataframe.geometry, geodataframe[column])
        ]
    else:
        shapes = [geom for geom in geodataframe.geometry]

    # shapes must be an iterable
    try:
        iter(shapes)
    except TypeError:
        shapes = (shapes,)

    raster = rasterio.features.rasterize(
        shapes,
        out_shape=like.shape,
        fill=fill,
        transform=imod.util.transform(like),
        **kwargs,
    )

    return xr.DataArray(raster, like.coords, like.dims)


def _handle_dtype(dtype, nodata):
    # Largely taken from rasterio.dtypes
    # https://github.com/mapbox/rasterio/blob/master/rasterio/dtypes.py
    # TODO: look at licensing of Mapbox license:
    # https://github.com/mapbox/rasterio/blob/master/LICENSE.txt
    # Not supported:
    # GDT_CInt16 = 8, GDT_CInt32 = 9, GDT_CFloat32 = 10, GDT_CFloat64 = 11
    dtype_mapping = {
        "uint8": 1,  # GDT_Byte
        "uint16": 2,  # GDT_Uint16
        "int16": 3,  # GDT_Int16
        "uint32": 4,  # GDT_Uint32
        "int32": 5,  # GDT_Int32
        "float32": 6,  # GDT_Float32
        "float64": 7,  # GDT_Float64
    }
    dtype_ranges = {
        "uint8": (0, 255),
        "uint16": (0, 65535),
        "int16": (-32768, 32767),
        "uint32": (0, 4294967295),
        "int32": (-2147483648, 2147483647),
        "float32": (-3.4028235e38, 3.4028235e38),
        "float64": (-1.7976931348623157e308, 1.7976931348623157e308),
    }

    def format_invalid(str_dtype):
        str_dtypes = dtype_mapping.keys()
        return "Invalid dtype: {0}, must be one of: {1}".format(
            str_dtype, ", ".join(str_dtypes)
        )

    if dtype is np.dtype(np.int64):
        dtype = np.int32

    str_dtype = str(np.dtype(dtype))
    if str_dtype not in dtype_mapping.keys():
        raise ValueError(format_invalid(str_dtype))
    gdal_dtype = dtype_mapping[str_dtype]

    if nodata is None:
        if np.issubdtype(dtype, np.integer):
            # Default to lowest value in case of integers
            nodata = dtype_ranges[str_dtype][0]
        elif np.issubdtype(dtype, np.floating):
            # Default to NaN in case of floats
            nodata = np.nan
    else:
        lower, upper = dtype_ranges[str_dtype]
        if nodata < lower or nodata > upper:
            raise ValueError(f"Nodata value {nodata} is out of bounds for {str_dtype}")

    return gdal_dtype, nodata


def gdal_rasterize(
    path, column, like=None, nodata=None, dtype=None, spatial_reference=None
):
    """
    Use GDAL to rasterize a vector file into an xarray.DataArray.

    Can be significantly more efficient than rasterize. This doesn't load the
    vector data into a GeoDataFrame and loops over the individual shapely
    geometries like rasterio.rasterize does, but loops over the features within
    GDAL instead.

    Parameters
    ----------
    path : str or pathlib.Path
        path to OGR supported vector file (e.g. a shapefile)
    column : str
        column name of column to burn into raster
    like : xr.DataArray, optional
        example of raster
    nodata : int, float; optional
    dtype : numpy.dtype, optional
    spatial_reference : dict, optional
        Optional dict to avoid allocating the like DataArray. Used if template
        is None. Dict has keys "bounds" and "cellsizes", with:

        * bounds = (xmin, xmax, ymin, ymax)
        * cellsizes = (dx, dy)

    Returns
    -------
    rasterized : np.array
    """
    from osgeo import gdal
    from osgeo import ogr

    if isinstance(path, pathlib.Path):
        p = path
        path = str(p)
    else:
        p = pathlib.Path(path)
    if not p.exists():
        raise FileNotFoundError(f"No such file: {path}")

    if dtype is None:
        if like is None:
            raise ValueError("If `like` is not provided, `dtype` has to be given")
        else:
            dtype = like.dtype
    gdal_dtype, nodata = _handle_dtype(dtype, nodata)

    # Exceptions will get raised on anything >= gdal.CE_Failure
    gdal.UseExceptions()

    # An attempt at decent errors
    class GdalErrorHandler:
        def __init__(self):
            self.err_level = gdal.CE_None
            self.err_no = 0
            self.err_msg = ""

        def handler(self, err_level, err_no, err_msg):
            self.err_level = err_level
            self.err_no = err_no
            self.err_msg = err_msg

    error = GdalErrorHandler()
    handler = error.handler
    gdal.PushErrorHandler(handler)

    # Get spatial data from template
    if like is not None:
        dx, xmin, _, dy, _, ymax = imod.util.spatial_reference(like)
        nrow, ncol = like.shape
    else:
        cellsizes = spatial_reference["cellsizes"]
        bounds = spatial_reference["bounds"]
        dx, dy = cellsizes
        if not isinstance(dx, (int, float)) or not isinstance(dy, (int, float)):
            raise ValueError("Cannot rasterize to a non-equidistant grid")
        coords = imod.util._xycoords(bounds, cellsizes)
        xmin, _, _, ymax = bounds
        nrow = coords["y"].size
        ncol = coords["x"].size
        dims = ("y", "x")

    # File will be closed when vector is dereferenced, after return
    vector = ogr.Open(path)
    vector_layer = vector.GetLayer()

    memory_driver = gdal.GetDriverByName("MEM")
    memory_raster = memory_driver.Create("", ncol, nrow, 1, gdal_dtype)
    memory_raster.SetGeoTransform([xmin, dx, 0, ymax, 0, dy])
    memory_band = memory_raster.GetRasterBand(1)
    memory_band.SetNoDataValue(nodata)
    memory_band.Fill(nodata)

    options = [f"ATTRIBUTE={column}"]
    gdal.RasterizeLayer(memory_raster, [1], vector_layer, None, None, [1], options)
    if error.err_level >= gdal.CE_Warning:
        reference_err_msg = (
            "Failed to fetch spatial reference on layer shape to build"
            " transformer, assuming matching coordinate systems."
        )
        if error.err_msg == reference_err_msg:
            pass
        else:
            raise RuntimeError("GDAL error: " + error.err_msg)

    if like is not None:
        rasterized = xr.full_like(like, memory_raster.ReadAsArray(), dtype=dtype)
    else:
        rasterized = xr.DataArray(memory_raster.ReadAsArray(), coords, dims)

    # Rasterio and GDAL’s bindings can contend for global GDAL objects
    # https://rasterio.readthedocs.io/en/stable/topics/switch.html#mutual-incompatibilities
    # TODO: Would this help at all?
    del gdal
    del ogr

    return rasterized


@numba.njit(cache=True)
def _cell_count(src, values, frequencies, nodata, *inds_weights):
    """
    numba compiled function to count the number of src cells occuring in the dst
    cells.

    Parameters
    ----------
    src : np.array
    values : np.array
        work array to store the unique values
    frequencies : np.array
        work array to store the tallied counts
    nodata : int, float
    inds_weights : tuple of np.arrays
        Contains indices of dst, indices of src, and weights.

    Returns
    -------
    tuple of np.arrays

       * row_indices
       * col_indices
       * values
       * frequencies
    """
    jj, blocks_iy, blocks_weights_y, kk, blocks_ix, blocks_weights_x = inds_weights

    # Use list for dynamic allocation, since we don't know number of rows in
    # advance.
    row_indices = []
    col_indices = []
    value_list = []
    count_list = []

    # j, k are indices of dst array
    # block_i contains indices of src array
    # block_w contains weights of src array
    for countj, j in enumerate(jj):
        block_iy = blocks_iy[countj]
        block_wy = blocks_weights_y[countj]
        for countk, k in enumerate(kk):
            block_ix = blocks_ix[countk]
            block_wx = blocks_weights_x[countk]

            # TODO: use weights in frequency count, and area sum?
            # Since src is equidistant, normed weights are easy to calculate.

            # Add the values and weights per cell in multi-dim block
            value_count = 0
            for iy, wy in zip(block_iy, block_wy):
                if iy < 0:
                    break
                for ix, wx in zip(block_ix, block_wx):
                    if ix < 0:
                        break

                    v = src[iy, ix]
                    if v == nodata:  # Skip nodata cells
                        continue
                    # Work on a single destination cell
                    # Count the number of polygon id's occuring in the cell
                    # a new row per id
                    found = False
                    for i in range(value_count):
                        if v == values[i]:
                            frequencies[i] += 1
                            found = True
                            break
                    if not found:
                        values[value_count] = v
                        frequencies[value_count] = 1
                        value_count += 1
                        # Add a new entry
                        row_indices.append(j)
                        col_indices.append(k)

            # Store for output
            value_list.extend(values[:value_count])
            count_list.extend(frequencies[:value_count])

            # reset storage
            values[:value_count] = 0
            frequencies[:value_count] = 0

    # Cast to numpy arrays
    row_i_arr = np.array(row_indices)
    col_i_arr = np.array(col_indices)
    value_arr = np.array(value_list)
    count_arr = np.array(count_list)

    return row_i_arr, col_i_arr, value_arr, count_arr


def _celltable(path, column, resolution, like):
    """
    Returns a table of cell indices (row, column) with feature ID, and feature
    area within cell. Essentially returns a COO sparse matrix, but with
    duplicate values per cell, since more than one geometry may be present.

    The feature area within the cell is approximated by first rasterizing the
    feature, and then counting the number of occuring cells. This means the
    accuracy of the area depends on the cellsize of the rasterization step.

    Parameters
    ----------
    path : str or pathlib.Path
        path to OGR supported vector file (e.g. a shapefile)
    column : str
        column name of column to burn into raster
    resolution : float
        cellsize at which the rasterization, and determination of area within
        cellsize occurs. Very small values are recommended (e.g. <= 0.5 m).
    like : xarray.DataArray
        Example DataArray of where the cells will be located. Used only for the
        coordinates.

    Returns
    -------
    cell_table : pandas.DataFrame
    """
    _, xmin, xmax, _, ymin, ymax = imod.util.spatial_reference(like)
    dx = resolution
    dy = -dx
    nodata = -1
    spatial_reference = {"bounds": (xmin, xmax, ymin, ymax), "cellsizes": (dx, dy)}

    rasterized = gdal_rasterize(
        path, column, nodata=nodata, dtype=np.int32, spatial_reference=spatial_reference
    )

    dst_coords = [imod.prepare.regrid._coord(like, dim) for dim in ("y", "x")]
    src_coords = [imod.prepare.regrid._coord(rasterized, dim) for dim in ("y", "x")]
    # Determine weights for every regrid dimension, and alloc_len,
    # the maximum number of src cells that may end up in a single dst cell
    inds_weights = []
    alloc_len = 1
    for src_x, dst_x in zip(src_coords, dst_coords):
        is_increasing = imod.prepare.regrid._is_increasing(src_x, dst_x)
        size, i_w = imod.prepare.regrid._weights_1d(src_x, dst_x, is_increasing)
        for elem in i_w:
            inds_weights.append(elem)
        alloc_len *= size

    # Pre-allocate work arrays
    values = np.full(alloc_len, 0)
    frequencies = np.full(alloc_len, 0)
    rows, cols, values, counts = _cell_count(
        rasterized.values, values, frequencies, nodata, *inds_weights
    )

    df = pd.DataFrame()
    df["row_index"] = rows
    df["col_index"] = cols
    df["id"] = values
    df["area"] = counts * (dx * dx)

    return df


def _create_chunks(like, resolution, chunksize):
    """
    Cuts data into chunksize by chunksize.

    Parameters
    ----------
    like : xarray.DataArray
    resolution : float
    chunksize : int

    Returns
    -------
    chunks : list of xr.DataArray
    """

    dx, xmin, xmax, dy, ymin, ymax = imod.util.spatial_reference(like)
    # Compute how many rows and columns are necessary for fine resolution
    nrow = int((ymax - ymin) / resolution)
    ncol = int((xmax - xmin) / resolution)
    # Number of rows and columns in a single chunk
    chunk_nrow = int(np.ceil(nrow / chunksize))
    chunk_ncol = int(np.ceil(ncol / chunksize))
    # Now compute how large these chunks are for the larger like cells
    height = int(chunk_nrow * (dy / resolution))
    width = int(chunk_ncol * (dx / resolution))

    chunks = []
    for i in range(chunk_nrow):
        for j in range(chunk_ncol):
            col_start = i * width
            col_end = (i + 1) * width
            row_start = i * height
            row_end = (i + 1) * height
            chunks.append(
                like.isel(y=slice(row_start, row_end), x=slice(col_start, col_end))
            )
    return chunks


def celltable(path, column, resolution, like, chunksize=1e4):
    """
    Process area of features by rasterizing in a chunkwise to limit memory
    usage.

    Returns a table of cell indices (row, column) with feature ID, and feature
    area within cell. Essentially returns a COO sparse matrix, but with
    duplicate values per cell, since more than one geometry may be present.

    The feature area within the cell is approximated by first rasterizing the
    feature, and then counting the number of occuring cells. This means the
    accuracy of the area depends on the cellsize of the rasterization step.

    Parameters
    ----------
    path : str or pathlib.Path
        path to OGR supported vector file (e.g. a shapefile)
    column : str
        column name of column to burn into raster
    resolution : float
        cellsize at which the rasterization, and determination of area within
        cellsize occurs. Very small values are recommended (e.g. <= 0.5 m).
    like : xarray.DataArray
        Example DataArray of where the cells will be located. Used only for the
        coordinates.
    chunksize : int, optional
        The size of the chunksize. Used for both x and y dimension.

    Returns
    -------
    celltable : pandas.DataFrame
    """
    like_chunks = _create_chunks(like, resolution, chunksize)
    collection = [
        dask.delayed(_celltable)(path, column, resolution, chunk)
        for chunk in like_chunks
    ]
    result = dask.compute(collection)[0]
    return pd.concat(result)


@numba.njit
def _burn_cells(raster, rows, cols, values):
    """
    Burn values of sparse COO-matrix into raster.
    rows, cols, and values form a sparse matrix in coordinate format (COO)
    (also known as "ijv" or "triplet" format).

    Parameters
    ----------
    raster : np.array
        raster to burn values into.
    rows : np.array of integers 
        row indices (i)
    cols : np.array of integers
        column indices (j)
    values : np.array of floats
        values to burn (v)
    """
    for i, j, v in zip(rows, cols, values):
        raster[i, j] = v
    return raster


def rasterize_celltable(table, column, like):
    """
    Rasterizes a table, such as produced by `imod.prepare.spatial.celltable`.
    Before rasterization, multiple values should be grouped and aggregated per
    cell. Values will be overwritten otherwise.

    Parameters
    ----------
    like : xr.DataArray
    table : pandas.DataFrame
        with columns: "row_index", "col_index"
    column : str, int, float
        column name of values to rasterize

    Returns
    -------
    rasterized : xr.DataArray
    """
    rows = table["row_index"].values
    cols = table["col_index"].values
    area = table[column].values
    dst = like.copy()
    dst.values = _burn_cells(dst.values, rows, cols, area)
    return dst

